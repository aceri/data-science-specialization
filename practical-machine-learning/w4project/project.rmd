---
title: "Human Activity Recognition using Wearable Sensors"
author: "David Pellon"
date: "7/5/2020"
output: html_document
---

### Introduction

This study tries to build a machine learning system to predict human activity using data from wearable technology sensors from HAR dataset.

     http://groupware.les.inf.puc-rio.br/har.

Files: 

     https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
     https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv


### Data Preprocessing


I have created some functions to help me process the data easier.
The steps will be a as follow:

    + Download the files
    + Removing Unneeded Columns
    + Fixing missing spaces
    + Removing Highly correlated columns


#### Downloading Data:

```{r b1, include=FALSE,fig.width=28,fig.height=20}
knitr::opts_chunk$set(echo = FALSE)

require(sqldf)
require(mice)
require(VIM)
require(caret)
require(randomForest)



check_NA <- function(df)
{
    t1 <- data.frame(num_NA=colSums(is.na(df))[colSums(is.na(df))>0])
    names(t1) <- c("num_NAs")
    return(t1)
}

csv1 <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
csv2 <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")

saveRDS(csv1,"csv1.rds")
saveRDS(csv2,"csv2.rds")

```

Details in the two original csv files (t1 and t2)


```{r b2, echo=FALSE,fig.width=28,fig.height=20}

writeLines(paste(
    "csv1 (first file)  - data set dimension : ",toString(dim(csv1)),
    "\ncsv2 (second file) - data set dimension : ",toString(dim(csv2)),sep=""))

```



#### Removing unneeded columns

Columns 1 to 7 do not seem to be useful here so I will remove them:

   + A row number
   + Timestamps
   + Windows

```{r b3, echo=FALSE,fig.width=28,fig.height=20}

head(csv1)[1:7]
csv1     <- csv1[,8:(dim(csv1)[2])]
csv2     <- csv2[,8:(dim(csv2)[2])]

```


Are both data files equal ?


```{r b4, echo=FALSE,fig.width=28,fig.height=20}

writeLines(paste("Columns on csv1 not included on csv2 : ",
    toString(names(csv1)[!(names(csv1) %in% names(csv2))]),
    "\nColumns on csv2 not included on csv1 : ",
    toString(names(csv2)[!(names(csv2) %in% names(csv1))]),sep=""))

alldata <- rbind(csv1[,1:(dim(csv1)[2]-1)],csv2[,1:(dim(csv2)[2]-1)])
alldata  <- alldata[,8:(dim(alldata)[2])]


```


#### Handling Missing Values

Let's plot the missing values for csv1

```{r b4b, echo=FALSE,fig.width=28,fig.height=20}

plot_histogram_pattern_missing_data <- function(df, df_name,num_slices=1,slice_num=1,plot=T)
{
    require(sqldf)
    k <- slice_num-1
    num_ini_cols=dim(df)[2]
    data <- df[,(num_ini_cols*k/num_slices):(num_ini_cols*(1+k)/num_slices)]
    if (plot == T) aggr(data,plot=T,ylab=c("Histogram of missing data","Pattern of missing rows")) else
    {
        aggr_plot <- aggr(data, numbers=TRUE, sortVars=TRUE, plot=FALSE, gap=3,numbers=T)$missings
        t1 <- sqldf(paste("select '",df_name,"' as Data_Set,count(Count) as Number_of_Columns,Count as Number_of_Missing_Rows from aggr_plot group by Count",sep=""))
        tab <- cbind(t1,nrow(df),round(t1$Number_of_Missing_Rows/nrow(df)*100,2),round(t1$Number_of_Columns/dim(df)[2]*100,2))
        names(tab)<-c("DataSet","Num_Cols","Num_Missing_Rows","Total_Rows","%_Missing_Rows","%_Cols_with_Missing_Rows")
        return(tab)
    }
}

plot_histogram_pattern_missing_data(csv1,"csv1",num_slices=1,plot=T)    

```
Now the  missing values for csv2

```{r b5, echo=FALSE,fig.width=28,fig.height=20}
plot_histogram_pattern_missing_data(csv2,"csv2 ",num_slices=1,plot=T)   
```

There are some columns on the csv2 file that have every single row missing (100%) 

Let's display it below in a table to confirm it:

```{r b6, echo=FALSE,fig.width=28,fig.height=20}


a1 <- plot_histogram_pattern_missing_data(csv1,   "csv1 ",num_slices=1,plot=F)
a2 <- plot_histogram_pattern_missing_data(csv2   ,"csv2 ",num_slices=1,plot=F)
rbind(a1,a2)
```

Seems that is the case.
We won't be able to impute any data or obtain anything from them. We will remove them.

```{r b7, echo=FALSE,fig.width=28,fig.height=20}

csv2NA <- rownames(check_NA(csv2))
csv2 <- csv2[,!(names(csv2) %in% csv2NA)]
csv1 <- csv1[,!(names(csv1) %in% csv2NA)]
alldata <- alldata[,!(names(alldata) %in% csv2NA)]

```

Seems everything is clear now:

```{r b8, echo=FALSE,fig.width=28,fig.height=20}

a1 <- plot_histogram_pattern_missing_data(csv1,   "csv1 ",num_slices=1,plot=F)
a2 <- plot_histogram_pattern_missing_data(csv2   ,"csv2 ",num_slices=1,plot=F)
rbind(a1,a2)

```

#### Handling High Correlated Columns


Still we could have very highly correlated columns. Using them might not be needed and could reduce efficiency of some methods like lda
We will try to find very highly correlated columns above 0.90 and remove them.



```{r b9, echo=FALSE, fig.width=28,fig.height=20}
list_Highly_Correlated <- function(df,max_correlation_preserved=0.90,verbose=0)
{
    ex_cor_df <- data.frame(col1=character(),col2=character(),corr=numeric(),other=character(),stringsAsFactors = FALSE)
    cont=0
    for (j in 1:(dim(df)[2]-1))
    {
        col1_name = names(df)[j]
        col1_data = df[,j]
        for (k in 1:(dim(df)[2]-1))
        {
            col2_name = names(df)[k]
            if (j!=k & !col2_name %in% ex_cor_df[,1])
            {
                col2_data = df[,k]
                corr=cor(col1_data,col2_data,use="complete.obs")            
                if (is.na(corr))
                {   cont=cont+1
                if(verbose>1) print(paste(cont," > Correlation of ",col1_name," [",j,"] with ",col2_name," [",k,"] is NA. Needs Review.",sep=""))
                ex_cor_df[cont,1]=col1_name
                ex_cor_df[cont,2]=col2_name
                ex_cor_df[cont,4]="NOT OK"
                } else { 
                    if (abs(corr) > max_correlation_preserved)
                    {
                        cont=cont+1
                        if(verbose>1) print(paste(cont," > Correlation of ",col1_name," [",j,"] with ",col2_name," [",k,"] is ",round(corr,2),sep=""))
                        ex_cor_df[cont,1]=col1_name
                        ex_cor_df[cont,2]=col2_name
                        ex_cor_df[cont,3]=round(corr,2)
                        ex_cor_df[cont,4]="OK"
                    }
                }
            }
        }
    }
    ex_cor_df$corr <- abs(ex_cor_df$corr)
    ex_cor_df <- ex_cor_df[order(ex_cor_df$corr),]
    HC_df  <- sqldf("select distinct(col1),corr as Highly_Correlated from ex_cor_df where other='OK'")
    HC_lst <- as.list(tail(HC_df,dim(HC_df)[1]-1))
    if (verbose >0) print(paste("Found ",nrow(HC_df)," highly correlated columns above defined threshold [x>",max_correlation_preserved,"]. Keeping the one from the list with the lowest correlation",sep=""))
    return(HC_lst) 
}

HC <- list_Highly_Correlated(df=csv1,max_correlation_preserved=0.90,verbose =1)
csv1 <- csv1[,!(names(csv1) %in% HC)]
csv2 <- csv2[,!(names(csv2) %in% HC)]

```

#### Partitioning the Data in csv1


```{r b10, echo=FALSE,fig.width=28,fig.height=20}

inTrain <- createDataPartition(y=csv1$classe,p=0.7,list=F)

training <- csv1[ inTrain,]
testing  <- csv1[-inTrain,] 

cbind(c("training","testing"),rbind(dim(training),dim(testing)))

```



### Creating 4 different Training Models & Predictions

Let's start with 4 different methodologies and then decide which one works best.

We will use the following methods :

+ Random Forest
+ Gradient Boosting Machine
+ LDA (Latent Dirichlet Allocation)
+ TreeBag (A type of bagging)


```{r b11, echo=FALSE,fig.width=28,fig.height=20}

mod01 <- randomForest(classe~.,data=training)
mod02 <- train(classe ~., data=training,method = "gbm"       ,verbose=F)
mod03 <- train(classe ~., data=training,method = "lda"       ,verbose=F)
mod04 <- train(classe ~., data=training,method = "treebag"   ,verbose=F)

pred01 <- predict(mod01,testing)
pred02 <- predict(mod02,testing)
pred03 <- predict(mod03,testing)
pred04 <- predict(mod04,testing)


```

### Prediction Results and Out of Sample

Let's compare their results and display Accuracy and Out of Sample Error:


```{r b12, echo=FALSE,fig.width=28,fig.height=20}

cm1 <- confusionMatrix(pred01, testing$classe)$overall['Accuracy']
cm2 <- confusionMatrix(pred02, testing$classe)$overall['Accuracy']
cm3 <- confusionMatrix(pred03, testing$classe)$overall['Accuracy']
cm4 <- confusionMatrix(pred04, testing$classe)$overall['Accuracy']

OutOfSample<-as.data.frame(cbind(c("rf","gbm","lda","treebag"),c(round(cm1,4),round(cm2,4),round(cm3,4),round(cm4,4)),rbind(round(1-cm1,4)*100,round(1-cm2,4)*100,round(1-cm3,4)*100,round(1-cm4,4)*100)))
names(OutOfSample)<-c("Model Method","Accuracy","Out_of_Sample_Error")
rownames(OutOfSample)<-NULL


OutOfSample


```

### Using Random Forest for final Prediction

Based on the results in the table above, the random forest model is very accurate.
We could create a model based on those four but the random forest is already very good.
The Treebag does it very well as well but as Random Forest internally performs Cross Validation to Estimate error rate and it's performance is the best already it gives more confidence.



Find below the random forest model prediction to the given values on csv2 file (the exercise) with new data.

```{r b13, echo=FALSE,fig.width=28,fig.height=20} 

predict(mod01,csv2)



```



















